## 代理池设计
代理池主要由三部分

1. 代理获取模块
   * 通过下载免费代理页面的html，分析得出所需要的代理信息存入数据库
2. REST API模块
   * 提供最基本的REST WEB API
3. 代理检测模块
   * 遍历数据库中的代理，通过访问指定的网页测试数据库中代理是否有效



## 目录结构说明

```
│                      
│  .gitignore      
│  requirements.txt             
├─docs									
│      设计文档.md
│      遇到的问题.md
│
├─log										# 存放日志的文件夹
├─proxy_pool
│  │  __init__.py
│  │
│  ├─api
│  │      proxy_api.py          			# REST WEB API(flask)
│  │      __init__.py
│  │
│  ├─config									# config
│  │      settings.py						# 工程settings文件
│  │      __init__.py
│  │
│  ├─db										# 数据库连接
│  │  │  client.py              			# 数据库连接抽象类
│  │  │  mysql_client.py					# MySQL数据库连接
│  │  │  __init__.py
│  │  │
│  │  └─model
│  │          proxy.py						# Proxy model
│  │          __init__.py
│  │
│  ├─proxy_getter							# 代理获取模块
│  │  │  proxy_fetch.py  					# 代理获取启动类
│  │  │  __init__.py
│  │  │
│  │  ├─getter
│  │  │      proxy_getter.py				# 代理获取抽象类
│  │  │      xicidaili_proxy_getter.py		# 西刺代理获取类
│  │  │      __init__.py
│  │  │
│  │  └─html
│  │          xicidaili.html				# 西刺代理网页html
│  │
│  ├─run									# 运行
│  │      run_by_apscheduler.py				# apscheduler多线程运行
│  │      run_by_process.py					# 多进程+apscheduler多进程运行
│  │      __init__.py
│  │
│  ├─utils									# 工具类
│  │      html_downloader.py				# html下载工具
│  │      logger.py							# 日志logger
│  │      __init__.py
│  │
│  └─validator
│          proxy_check_thread.py			# 代理建议线程
│          raw_proxy_check.py				# 原始代理有效性检测
│          valid_proxy_check.py				# 有效代理有效性检测
│          __init__.py
│
└─tests										# 单元测试

```



## 扩展

### 扩展代理

　　项目默认包含几个免费的代理获取方法，但是免费的毕竟质量不好，所以如果直接运行可能拿到的代理质量不理想。所以，提供了代理获取的扩展方法。

　　添加一个新的代理获取方法如下:

- 1、首先在proxy_pool/proxy_getter/getter文件夹中创建对于的xxxx_proxy_getter.py，在文件中创建对应的类XXXXProxyGetter，继承proxy_pool/proxy_getter/getter/proxy_getter/proxy_getter.py中的ProxyGetter代理获取抽象类，并实现get_proxy抽象方法

```
class XicidailiProxyGetter(ProxyGetter):
    # 透明URL
    transparent_url = 'http://www.xicidaili.com/nn/'
    # 高匿URL
    anonymous_url = 'http://www.xicidaili.com/nt/'

    @classmethod
    def get_proxy(cls):
        """
        爬取西刺代理网站html，解析后获得代理将代理加入数据库
        """
        request = HtmlDownloader()
        proxy_getter_logger.info("Start get proxy")
        # 爬取透明proxy
        for page in range(1, 5):
            # 拼接url
            url = cls.transparent_url + str(page)
            html = request.download(url=url)
            # 从html中提取proxy
            soup = BeautifulSoup(html, features="html.parser")
            table = soup.find("table", id="ip_list")
            for tr in table.find_all("tr")[1:]:
                td_list = tr.find_all("td")
                proxy_dict = {
                    "country": "China",
                    "ip": unicode(td_list[1].string),
                    "port": int(td_list[2].string),
                    "area": unicode(td_list[3].find("a").string if td_list[3].find("a") else ""),
                    "type": TRANSPARENT,
                    "protocol": HTTPS_PROTOCOL if td_list[5].string == "HTTPS" else HTTP_PROTOCOL,
                    "is_valid": False
                }
                proxy_getter_logger.info(proxy_dict)
                Client.insert(proxy_dict)
            # 防止被代理网站反爬，添加间隔时间2s
            time.sleep(1)

        # 爬取高匿proxy
        for page in range(1, 5):
            # 拼接url
            url = cls.anonymous_url + str(page)
            html = request.download(url=url)
            # 从html中提取proxy
            soup = BeautifulSoup(html, features="html.parser")
            table = soup.find("table", id="ip_list")
            for tr in table.find_all("tr")[1:]:
                td_list = tr.find_all("td")
                proxy_dict = {
                    "country": "China",
                    "ip": unicode(td_list[1].string),
                    "port": int(td_list[2].string),
                    "area": unicode(td_list[3].find("a").string if td_list[3].find("a") else ""),
                    "type": ANONYMOUS,
                    "protocol": HTTPS_PROTOCOL if td_list[5].string == "HTTPS" else HTTP_PROTOCOL,
                    "is_valid": False
                }
                proxy_getter_logger.info(proxy_dict)
                Client.insert(proxy_dict)
            # 防止被代理网站反爬，添加间隔时间2s
            time.sleep(1)

```

- 2、在proxy_pool/proxy_getter/getter/proxy_getter/\__init__.py中的\__all__后加入XXXXProxyGetter

  ```python
  from xicidaili_proxy_getter import XicidailiProxyGetter
  from xxxx_proxy_getter import XXXXProxyGetter
  
  __all__ = XicidailiProxyGetter,XXXXProxyGetter
  
  ```

- 3、修改proxy_pool/config/settings.py文件中的`PROXY_GETTER`项：

　　在`PROXY_GETTER`下添加XXXXProxyGetter的名字:

```
PROXY_GETTER = [
   "XXXXProxyGetter"
]
```